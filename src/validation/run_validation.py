"""
Integrated Validation Script

Runs complete validation of both core assumptions:
1. Graph structure can capture Laplacian dynamics
2. Laplacian dynamics holds in financial markets
"""

import numpy as np
import matplotlib.pyplot as plt
from typing import Dict, List
import sys
import os

# Add parent directory to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from synthetic_data import generate_complete_dataset, visualize_synthetic_data
from graph_learning import correlation_graph, dynamics_based_graph, topk_sparsify
from dynamics_test import (
    test_force_return_correlation,
    test_dynamics_regression,
    test_prediction_accuracy,
    test_graph_stability,
    visualize_dynamics_test_results
)
from metrics import evaluate_graph_recovery, print_evaluation_report


def run_assumption1_validation(
    n_stocks: int = 50,
    n_clusters: int = 5,
    n_timesteps: int = 1000,
    seed: int = 42,
    dt: float = 0.01,
    noise_std: float = 0.01,
    noise_type: str = 'gaussian'
) -> Dict:
    """
    éªŒè¯å‡è®¾1ï¼šå›¾ç»“æ„èƒ½æ•æ‰æ‹‰æ™®æ‹‰æ–¯åŠ¨åŠ›å­¦

    æµ‹è¯•ï¼š
    - èƒ½å¦ä»åˆæˆæ•°æ®ä¸­æ¢å¤çœŸå®å›¾ç»“æ„
    - ä¸åŒæ–¹æ³•çš„å¯¹æ¯”

    Returns:
        results: éªŒè¯ç»“æœå­—å…¸
    """
    print("\n" + "=" * 70)
    print("å‡è®¾1éªŒè¯ï¼šå›¾ç»“æ„å­¦ä¹ èƒ½åŠ›")
    print("=" * 70)

    # 1. ç”Ÿæˆåˆæˆæ•°æ®
    print("\n[1/4] ç”Ÿæˆåˆæˆæ•°æ®...")
    dataset = generate_complete_dataset(
        n_stocks=n_stocks,
        n_clusters=n_clusters,
        n_timesteps=n_timesteps,
        seed=seed,
        dt=dt,
        noise_std=noise_std,
        noise_type=noise_type
    )

    print(f"  âœ“ ç”Ÿæˆ {n_stocks} åªè‚¡ç¥¨ï¼Œ{n_clusters} ä¸ªç°‡ï¼Œ{n_timesteps} ä¸ªæ—¶é—´æ­¥")
    print(f"  âœ“ çœŸå®å›¾å¯†åº¦: {(dataset['A_true'] > 0).sum() / n_stocks**2:.2%}")

    f_series = dataset['f_series']
    A_true = dataset['A_true']
    cluster_labels = dataset['cluster_labels']

    # 2. æµ‹è¯•ä¸åŒå›¾å­¦ä¹ æ–¹æ³•
    print("\n[2/4] æµ‹è¯•å›¾å­¦ä¹ æ–¹æ³•...")

    methods = {}

    # æ–¹æ³•1ï¼šç›¸å…³æ€§å›¾ï¼ˆBaselineï¼‰
    print("  - ç›¸å…³æ€§å›¾...")
    A_corr = correlation_graph(f_series, threshold=0.3)
    methods['correlation'] = {
        'A': A_corr,
        'name': 'ç›¸å…³æ€§å›¾',
    }

    # æ–¹æ³•2ï¼šåŠ¨åŠ›å­¦å›¾ï¼ˆBaselineï¼‰
    print("  - åŠ¨åŠ›å­¦å›¾ (Baseline)...")
    A_dyn, L_dyn = dynamics_based_graph(f_series, method='regression', alpha=0.01)
    methods['dynamics'] = {
        'A': A_dyn,
        'L': L_dyn,
        'name': 'åŠ¨åŠ›å­¦å›¾(Baseline)',
    }

    # æ–¹æ³•3ï¼šGNNå­¦ä¹ å›¾ï¼ˆä¸»è¦æ–¹æ³•ï¼‰
    print(f"  - GNNå­¦ä¹ å›¾ (Robust Training)...")
    from gnn_training import train_gnn_model, predict_graph

    # è®­ç»ƒGNN
    print("    å¼€å§‹è®­ç»ƒGNN...")
    gnn_model, losses = train_gnn_model(
        f_series,
        n_epochs=50,  # å¿«é€ŸéªŒè¯ç”¨50è½®
        verbose=False
    )

    # é¢„æµ‹å›¾
    A_gnn = predict_graph(gnn_model, f_series)

    # ç®€å•çš„åå¤„ç†ï¼ˆTop-Kç¨€ç–åŒ–ï¼Œå› ä¸ºGNNè¾“å‡ºå…¨è¿æ¥æƒé‡ï¼‰
    A_gnn_sparse = topk_sparsify(A_gnn, k=n_stocks // n_clusters * 2)

    # è®¡ç®—å¯¹åº”çš„L
    degree = A_gnn_sparse.sum(axis=1)
    L_gnn = np.diag(degree) - A_gnn_sparse

    methods['gnn'] = {
        'A': A_gnn_sparse,
        'L': L_gnn,
        'name': 'GNN (Ours)',
    }

    # 3. è¯„ä¼°æ¯ä¸ªæ–¹æ³•
    print("\n[3/4] è¯„ä¼°å›¾æ¢å¤è´¨é‡...")

    results = {}
    for method_name, method_data in methods.items():
        print(f"\n  === {method_data['name']} ===")
        metrics = evaluate_graph_recovery(
            method_data['A'],
            A_true,
            cluster_labels,
            threshold=0.1
        )
        results[method_name] = metrics
        print_evaluation_report(metrics, method_data['name'])

    # 4. å¯¹æ¯”æ€»ç»“
    print("\n[4/4] å¯¹æ¯”æ€»ç»“")
    print("\n" + "-" * 80)
    print(f"{'æ–¹æ³•':<20} {'F1':<10} {'NMI(ç°‡)':<10} {'ARI(ç°‡)':<10} {'Purity':<10}")
    print("-" * 80)

    for method_name, metrics in results.items():
        method_display = methods[method_name]['name']
        f1 = metrics['edge_f1']
        nmi = metrics['nmi']
        ari = metrics['ari']
        purity = metrics['purity']
        print(f"{method_display:<20} {f1:<10.4f} {nmi:<10.4f} {ari:<10.4f} {purity:<10.4f}")

    print("-" * 80)

    # æ›´æ–°å†³ç­–é€»è¾‘
    results['dataset'] = dataset
    results['methods'] = methods

    return results


def run_assumption2_validation(
    dataset: Dict,
    L: np.ndarray,
    method_name: str = "åŠ¨åŠ›å­¦å›¾"
) -> Dict:
    """
    éªŒè¯å‡è®¾2ï¼šæ‹‰æ™®æ‹‰æ–¯åŠ¨åŠ›å­¦åœ¨æ•°æ®ä¸­æˆç«‹

    æµ‹è¯•ï¼š
    - æ¢å¤åŠ›æ˜¯å¦é¢„æµ‹æœªæ¥æ”¶ç›Š
    - åŠ¨åŠ›å­¦æ–¹ç¨‹çš„æ‹Ÿåˆä¼˜åº¦
    - é¢„æµ‹å‡†ç¡®æ€§

    Args:
        dataset: æ•°æ®é›†ï¼ˆæ¥è‡ªassumption1ï¼‰
        L: æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µ
        method_name: æ–¹æ³•åç§°

    Returns:
        results: éªŒè¯ç»“æœå­—å…¸
    """
    print("\n" + "=" * 70)
    print(f"å‡è®¾2éªŒè¯ï¼šæ‹‰æ™®æ‹‰æ–¯åŠ¨åŠ›å­¦æœ‰æ•ˆæ€§ï¼ˆä½¿ç”¨{method_name}ï¼‰")
    print("=" * 70)

    f_series = dataset['f_series']

    results = {}

    # Test 1: Force-return correlation
    print("\n[1/4] æ¢å¤åŠ›-æ”¶ç›Šç›¸å…³æ€§æµ‹è¯•...")
    ic_results = test_force_return_correlation(f_series, L, lag=1)
    results['ic_test'] = ic_results

    print(f"  ICå‡å€¼:     {ic_results['ic_mean']:.6f}")
    print(f"  ICæ ‡å‡†å·®:   {ic_results['ic_std']:.6f}")
    print(f"  ä¿¡æ¯æ¯”ç‡:   {ic_results['ic_ir']:.4f}")
    print(f"  tç»Ÿè®¡é‡:    {ic_results['t_statistic']:.3f}")
    print(f"  på€¼:        {ic_results['p_value']:.6f}")
    print(f"  èƒœç‡:       {ic_results['win_rate']:.2%}")

    if ic_results['p_value'] < 0.05:
        print(f"  âœ… æ˜¾è‘—æ€§: é€šè¿‡ (p < 0.05)")
    else:
        print(f"  âŒ æ˜¾è‘—æ€§: æœªé€šè¿‡ (p >= 0.05)")

    # Test 2: Dynamics regression
    print("\n[2/4] åŠ¨åŠ›å­¦å›å½’æµ‹è¯•...")
    reg_results = test_dynamics_regression(f_series, L, dt=dataset['dt'])
    results['regression_test'] = reg_results

    print(f"  Betaç³»æ•°:   {reg_results['beta']:.4f} (åº”æ¥è¿‘1)")
    print(f"  RÂ²:         {reg_results['r_squared']:.4f}")
    print(f"  å¹³å‡RÂ²:     {reg_results['r_squared_mean']:.4f}")

    if reg_results['r_squared'] > 0.5:
        print(f"  âœ… æ‹Ÿåˆä¼˜åº¦: è‰¯å¥½ (RÂ² > 0.5)")
    else:
        print(f"  âš ï¸  æ‹Ÿåˆä¼˜åº¦: ä¸€èˆ¬ (RÂ² < 0.5)")

    # Test 3: Prediction accuracy
    print("\n[3/4] é¢„æµ‹å‡†ç¡®æ€§æµ‹è¯•...")
    pred_results = test_prediction_accuracy(f_series, L, prediction_horizon=5, dt=dataset['dt'])
    results['prediction_test'] = pred_results

    print(f"  MSE:        {pred_results['mse_mean']:.6f}")
    print(f"  MAE:        {pred_results['mae_mean']:.6f}")
    print(f"  æ–¹å‘å‡†ç¡®ç‡: {pred_results['direction_accuracy']:.2%}")

    if pred_results['direction_accuracy'] > 0.55:
        print(f"  âœ… æ–¹å‘é¢„æµ‹: ä¼˜äºéšæœº (> 55%)")
    else:
        print(f"  âŒ æ–¹å‘é¢„æµ‹: æœªä¼˜äºéšæœº (< 55%)")

    # Test 4: Summary
    print("\n[4/4] ç»¼åˆè¯„ä¼°")
    print("\n" + "-" * 70)

    # åˆ¤æ–­æ˜¯å¦é€šè¿‡
    passed_tests = []
    failed_tests = []

    if ic_results['p_value'] < 0.05 and ic_results['ic_mean'] > 0.05:
        passed_tests.append("âœ… ICæ˜¾è‘—ä¸”ä¸ºæ­£")
    else:
        failed_tests.append(f"âŒ IC = {ic_results['ic_mean']:.4f}, p = {ic_results['p_value']:.4f}")

    if reg_results['r_squared'] > 0.5:
        passed_tests.append("âœ… åŠ¨åŠ›å­¦æ‹Ÿåˆè‰¯å¥½")
    else:
        failed_tests.append(f"âŒ RÂ² = {reg_results['r_squared']:.4f} < 0.5")

    if pred_results['direction_accuracy'] > 0.55:
        passed_tests.append("âœ… æ–¹å‘é¢„æµ‹ä¼˜äºéšæœº")
    else:
        failed_tests.append(f"âŒ æ–¹å‘å‡†ç¡®ç‡ = {pred_results['direction_accuracy']:.2%}")

    print("é€šè¿‡çš„æµ‹è¯•:")
    for test in passed_tests:
        print(f"  {test}")

    if failed_tests:
        print("\næœªé€šè¿‡çš„æµ‹è¯•:")
        for test in failed_tests:
            print(f"  {test}")

    print("-" * 70)

    # å¯è§†åŒ–
    print("\nç”Ÿæˆå¯è§†åŒ–...")
    visualize_dynamics_test_results(ic_results)

    return results


def run_complete_validation(
    n_stocks: int = 50,
    n_clusters: int = 5,
    n_timesteps: int = 1000,
    seed: int = 42,
    dt: float = 0.01,
    noise_std: float = 0.01,
    noise_type: str = 'gaussian'
) -> Dict:
    """
    è¿è¡Œå®Œæ•´çš„éªŒè¯æµç¨‹

    Returns:
        complete_results: åŒ…å«æ‰€æœ‰éªŒè¯ç»“æœçš„å­—å…¸
    """
    print("\n" + "=" * 70)
    print("æ·±åº¦åŠ¿èƒ½å¥—åˆ© - æ ¸å¿ƒå‡è®¾éªŒè¯")
    print("=" * 70)
    print(f"\nå‚æ•°è®¾ç½®:")
    print(f"  è‚¡ç¥¨æ•°é‡: {n_stocks}")
    print(f"  ç°‡æ•°é‡:   {n_clusters}")
    print(f"  æ—¶é—´æ­¥æ•°: {n_timesteps}")
    print(f"  éšæœºç§å­: {seed}")
    print(f"  å™ªå£°åˆ†å¸ƒ: {noise_type}")

    # å‡è®¾1éªŒè¯
    assumption1_results = run_assumption1_validation(
        n_stocks=n_stocks,
        n_clusters=n_clusters,
        n_timesteps=n_timesteps,
        seed=seed,
        dt=dt,
        noise_std=noise_std,
        noise_type=noise_type
    )

    # å‡è®¾2éªŒè¯ï¼ˆä½¿ç”¨GNNå­¦ä¹ çš„å›¾ - å¦‚æœGNNå­¦å¾—å¥½ï¼ŒåŠ¨åŠ›å­¦åº”è¯¥æ›´æ˜¾è‘—ï¼‰
    dataset = assumption1_results['dataset']

    # ä¼˜å…ˆä½¿ç”¨GNNçš„ç»“æœï¼Œå¦‚æœæ²¡æœ‰åˆ™å›é€€åˆ°dynamics
    if 'gnn' in assumption1_results['methods']:
        L_best = assumption1_results['methods']['gnn']['L']
        method_name = "GNNå­¦ä¹ å›¾"
    else:
        L_best = assumption1_results['methods']['dynamics']['L']
        method_name = "åŠ¨åŠ›å­¦å›¾(Baseline)"

    assumption2_results = run_assumption2_validation(
        dataset=dataset,
        L=L_best,
        method_name=method_name
    )

    # æœ€ç»ˆå†³ç­–
    print("\n" + "=" * 70)
    print("æœ€ç»ˆå†³ç­–")
    print("=" * 70)

    # å‡è®¾1é€šè¿‡æ ‡å‡†ï¼ˆæ›´æ–°ï¼‰
    # é‡ç‚¹å…³æ³¨ç°‡è´¨é‡æŒ‡æ ‡
    if 'gnn' in assumption1_results['methods']:
        gnn_metrics = assumption1_results['gnn']
        assumption1_passed = (gnn_metrics['nmi'] > 0.7 and gnn_metrics['ari'] > 0.6)

        print(f"\nå‡è®¾1ï¼ˆGNNèšç±»èƒ½åŠ›ï¼‰: {'âœ… é€šè¿‡' if assumption1_passed else 'âŒ æœªé€šè¿‡'}")
        print(f"  - NMI = {gnn_metrics['nmi']:.4f} {'>' if gnn_metrics['nmi'] > 0.7 else '<'} 0.7")
        print(f"  - ARI = {gnn_metrics['ari']:.4f} {'>' if gnn_metrics['ari'] > 0.6 else '<'} 0.6")
        print(f"  - Purity = {gnn_metrics['purity']:.4f}")
    else:
        # Fallback logic
        assumption1_passed = False
        print("\nå‡è®¾1: âŒ æœªé€šè¿‡ (GNNæœªè¿è¡Œ)")

    # å‡è®¾2é€šè¿‡æ ‡å‡†
    ic_test = assumption2_results['ic_test']
    assumption2_passed = (
        ic_test['p_value'] < 0.05 and
        ic_test['ic_mean'] > 0.05
    )

    print(f"\nå‡è®¾2ï¼ˆåŠ¨åŠ›å­¦å­˜åœ¨æ€§ï¼‰: {'âœ… é€šè¿‡' if assumption2_passed else 'âŒ æœªé€šè¿‡'}")
    print(f"  - IC = {ic_test['ic_mean']:.6f} {'>' if ic_test['ic_mean'] > 0.05 else '<'} 0.05")
    print(f"  - på€¼ = {ic_test['p_value']:.6f} {'<' if ic_test['p_value'] < 0.05 else '>'} 0.05")

    print("\n" + "-" * 70)

    if assumption1_passed and assumption2_passed:
        print("ğŸŸ¢ ç»¿ç¯ï¼šä¸¤ä¸ªå‡è®¾éƒ½é€šè¿‡ï¼Œå»ºè®®ç»§ç»­å¼€å‘å®Œæ•´ç³»ç»Ÿ")
    elif assumption1_passed or assumption2_passed:
        print("ğŸŸ¡ é»„ç¯ï¼šä¸€ä¸ªå‡è®¾é€šè¿‡ï¼Œå»ºè®®ä¿®æ­£åé‡æ–°éªŒè¯")
    else:
        print("ğŸ”´ çº¢ç¯ï¼šä¸¤ä¸ªå‡è®¾éƒ½æœªé€šè¿‡ï¼Œå»ºè®®é‡æ–°æ€è€ƒæ ¸å¿ƒæ–¹æ³•")

    print("=" * 70)

    return {
        'assumption1': assumption1_results,
        'assumption2': assumption2_results,
        'decision': {
            'assumption1_passed': assumption1_passed,
            'assumption2_passed': assumption2_passed,
            'overall': 'green' if (assumption1_passed and assumption2_passed) else
                      'yellow' if (assumption1_passed or assumption2_passed) else
                      'red'
        }
    }


# ============================================================================
# ä¸»ç¨‹åº
# ============================================================================

if __name__ == "__main__":
    # è¿è¡Œå®Œæ•´éªŒè¯
    results = run_complete_validation(
        n_stocks=50,
        n_clusters=5,
        n_timesteps=1000,
        seed=42,
        dt=0.01,
        noise_std=0.01,
        noise_type='student-t' # Run 4: Heavy Tails
    )

    # ä¿å­˜ç»“æœï¼ˆå¯é€‰ï¼‰
    # np.savez('validation_results.npz', **results)
    # print("\nç»“æœå·²ä¿å­˜åˆ°: validation_results.npz")
