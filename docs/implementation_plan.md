# Implementation Plan: Core Assumption Validation (Updated)

**Date:** 2025-12-10
**Goal:** Validate the two fundamental assumptions of the Deep Potential Arbitrage framework

---

## ðŸŽ¯ æ ¸å¿ƒé€»è¾‘é“¾æ¡

### æˆ‘ä»¬çš„æ–¹æ³•è®º

```
è‚¡ç¥¨å¸‚åœº â†’ å­˜åœ¨å‡å€¼å›žå½’ (OU process) â†’ å¯ç”¨æ‹‰æ™®æ‹‰æ–¯åŠ¨åŠ›å­¦å»ºæ¨¡ â†’ GNNå­¦ä¹ èšç±»ç»“æž„ â†’ Controlleråˆ©ç”¨åŠ¿èƒ½æ¢¯åº¦äº¤æ˜“
```

### å…³é”®å‡è®¾

1. **å‡è®¾1ï¼šGNNèƒ½æ•æ‰æ‹‰æ™®æ‹‰æ–¯åŠ¨åŠ›å­¦å½¢æˆçš„èšç±»å…³ç³»**
   - æ‹‰æ™®æ‹‰æ–¯åŠ¨åŠ›å­¦ä¼šè‡ªç„¶å½¢æˆåŒæ­¥ç°‡
   - GNNèƒ½å¤Ÿä»Žæ•°æ®ä¸­å­¦ä¹ åˆ°è¿™äº›ç°‡
   - å­¦åˆ°çš„ç°‡ä¸ŽçœŸå®žçš„åŠ¨åŠ›å­¦ç°‡ä¸€è‡´

2. **å‡è®¾2ï¼šè‚¡ç¥¨å¸‚åœºå­˜åœ¨æ‹‰æ™®æ‹‰æ–¯åŠ¨åŠ›å­¦**
   - è‚¡ç¥¨ä»·æ ¼éµå¾ªç±»ä¼¼OUè¿‡ç¨‹çš„å‡å€¼å›žå½’
   - è¿™ç§å›žå½’å¯ä»¥ç”¨æ‹‰æ™®æ‹‰æ–¯åŠ¨åŠ›å­¦å»ºæ¨¡
   - æ¢å¤åŠ›èƒ½é¢„æµ‹æœªæ¥ä»·æ ¼å˜åŒ–

---

## âš ï¸ é‡è¦æ¾„æ¸…

### âŒ æˆ‘ä»¬ä¸åšä»€ä¹ˆ

**ä¸æ˜¯"åŠ¨åŠ›å­¦åæŽ¨å›¾"ï¼š**
- æˆ‘ä»¬ä¸ä¼šç”¨ `dynamics_based_graph()` ä½œä¸ºä¸»è¦æ–¹æ³•
- é‚£åªæ˜¯ä¸€ä¸ªbaselineï¼Œç”¨æ¥éªŒè¯æ¦‚å¿µ
- å®žé™…ç³»ç»Ÿä¸­ï¼Œæˆ‘ä»¬ç”¨**GNNå­¦ä¹ å›¾**

### âœ… æˆ‘ä»¬åšä»€ä¹ˆ

**GNNå­¦ä¹ èšç±»ï¼š**
- ç”¨GNNä»Žæ—¶é—´åºåˆ—æ•°æ®ä¸­å­¦ä¹ å›¾ç»“æž„
- éªŒè¯GNNå­¦åˆ°çš„ç°‡ä¸Žæ‹‰æ™®æ‹‰æ–¯åŠ¨åŠ›å­¦å½¢æˆçš„ç°‡ä¸€è‡´
- è¯æ˜ŽGNNèƒ½æ•æ‰åˆ°åŠ¨åŠ›å­¦çš„æœ¬è´¨

---

## éªŒè¯ç­–ç•¥

### é˜¶æ®µ1ï¼šåˆæˆæ•°æ®éªŒè¯ï¼ˆControlled Experimentï¼‰
- ç”Ÿæˆå·²çŸ¥æ‹‰æ™®æ‹‰æ–¯åŠ¨åŠ›å­¦çš„æ•°æ®
- ç”¨GNNå­¦ä¹ å›¾ç»“æž„
- å¯¹æ¯”GNNå­¦åˆ°çš„ç°‡ vs çœŸå®žåŠ¨åŠ›å­¦ç°‡

### é˜¶æ®µ2ï¼šçœŸå®žæ•°æ®éªŒè¯ï¼ˆReal-World Testï¼‰
- ä½¿ç”¨çœŸå®žè‚¡ç¥¨æ•°æ®
- æµ‹è¯•æ˜¯å¦å­˜åœ¨æ‹‰æ™®æ‹‰æ–¯åŠ¨åŠ›å­¦
- éªŒè¯GNNèƒ½å¦å‘çŽ°è¿™ç§ç»“æž„

---

## Part 1: å‡è®¾1éªŒè¯ - GNNèƒ½å­¦ä¹ æ‹‰æ™®æ‹‰æ–¯åŠ¨åŠ›å­¦èšç±»

### 1.1 åˆæˆæ•°æ®ç”Ÿæˆï¼ˆä¿æŒä¸å˜ï¼‰

ä½¿ç”¨ `synthetic_data.py` ç”Ÿæˆç¬¦åˆæ‹‰æ™®æ‹‰æ–¯åŠ¨åŠ›å­¦çš„æ•°æ®ï¼š
```python
dataset = generate_complete_dataset(
    n_stocks=50,
    n_clusters=5,
    n_timesteps=1000
)
# åŒ…å«ï¼š
# - A_true: çœŸå®žå›¾ç»“æž„
# - cluster_labels: çœŸå®žç°‡æ ‡ç­¾
# - f_series: çŠ¶æ€æ¼”åŒ–ï¼ˆç¬¦åˆ df/dt = -Lfï¼‰
```

---

### 1.2 GNNå›¾å­¦ä¹ ï¼ˆæ ¸å¿ƒæ–¹æ³•ï¼‰

**ç›®æ ‡ï¼š** ç”¨GNNä»Žæ—¶é—´åºåˆ—ä¸­å­¦ä¹ å›¾ç»“æž„

#### æ–¹æ³•Aï¼šç®€å•GNNï¼ˆå…ˆå®žçŽ°è¿™ä¸ªï¼‰

```python
class SimpleGNN(nn.Module):
    """
    ç®€å•çš„GNNå›¾å­¦ä¹ å™¨

    è¾“å…¥ï¼šæ—¶é—´åºåˆ— X (N, T, d)
    è¾“å‡ºï¼šé‚»æŽ¥çŸ©é˜µ A (N, N)
    """
    def __init__(self, d_input, d_hidden, n_stocks):
        super().__init__()
        # æ—¶åºç¼–ç å™¨
        self.temporal_encoder = nn.Conv1d(d_input, d_hidden, kernel_size=5)

        # èŠ‚ç‚¹åµŒå…¥
        self.node_embedding = nn.Linear(d_hidden, d_hidden)

        # è¾¹é¢„æµ‹å™¨ï¼ˆpairwiseï¼‰
        self.edge_predictor = nn.Bilinear(d_hidden, d_hidden, 1)

    def forward(self, X):
        # X: (N, T, d)
        # 1. æ—¶åºç¼–ç 
        h = self.temporal_encoder(X.transpose(1, 2))  # (N, d_hidden, T')
        h = h.mean(dim=2)  # (N, d_hidden) - æ± åŒ–

        # 2. èŠ‚ç‚¹åµŒå…¥
        h = self.node_embedding(h)  # (N, d_hidden)

        # 3. è¾¹é¢„æµ‹ï¼ˆæ‰€æœ‰é…å¯¹ï¼‰
        N = len(h)
        A = torch.zeros(N, N)
        for i in range(N):
            for j in range(i+1, N):
                score = self.edge_predictor(h[i], h[j])
                A[i, j] = A[j, i] = torch.sigmoid(score)

        return A
```

#### æ–¹æ³•Bï¼šå›¾ç»“æž„å­¦ä¹ ï¼ˆå¯é€‰ï¼Œæ›´é«˜çº§ï¼‰

ä½¿ç”¨å¯å¾®åˆ†çš„å›¾å­¦ä¹ æ–¹æ³•ï¼ˆå¦‚LDS, NRIç­‰ï¼‰

---

### 1.3 è®­ç»ƒç›®æ ‡

**æ ¸å¿ƒé—®é¢˜ï¼š** ç”¨ä»€ä¹ˆlossè®­ç»ƒGNNï¼Ÿ

#### é€‰é¡¹1ï¼šç›‘ç£å­¦ä¹ ï¼ˆå¦‚æžœæœ‰çœŸå®žå›¾ï¼‰

```python
# åœ¨åˆæˆæ•°æ®ä¸Šï¼Œæˆ‘ä»¬çŸ¥é“çœŸå®žå›¾A_true
loss = binary_cross_entropy(A_pred, A_true)
```

**ä¼˜ç‚¹ï¼š** ç›´æŽ¥ï¼Œå®¹æ˜“æ”¶æ•›
**ç¼ºç‚¹ï¼š** çœŸå®žæ•°æ®æ²¡æœ‰ground truth

#### é€‰é¡¹2ï¼šåŠ¨åŠ›å­¦ä¸€è‡´æ€§ï¼ˆæŽ¨è - é²æ£’å‡çº§ç‰ˆï¼‰

ä¸ºäº†ä¸Ž Controller v3.1 çš„é²æ£’è®¾è®¡ä¿æŒä¸€è‡´ï¼Œæ¨¡åž‹è®­ç»ƒä¹Ÿå¿…é¡»**é²æ£’åŒ–**ã€‚æˆ‘ä»¬ä¸èƒ½è®© GNN ä¸ºäº†æ‹Ÿåˆä¸€ä¸ªå·¨å¤§çš„é»‘å¤©é¹…ï¼ˆOutlierï¼‰è€Œç ´åäº†æ•´ä¸ªå›¾ç»“æž„ã€‚

```python
# å­¦åˆ°çš„å›¾åº”è¯¥èƒ½è§£é‡ŠåŠ¨åŠ›å­¦ï¼Œä½†è¦å¿½ç•¥é»‘å¤©é¹…
# df/dt â‰ˆ -L * f

def robust_dynamics_loss(A_pred, f_series, delta=1.0):
    L_pred = compute_laplacian(A_pred)

    # è®¡ç®— df/dt
    df_dt = (f_series[1:] - f_series[:-1]) / dt

    # é¢„æµ‹çš„åŠ›ï¼šF_pred = -L * f
    force_pred = -L_pred @ f_series[:-1].T

    # é¢„æµ‹è¯¯å·®ï¼šResidual = df/dt - F_pred
    residual = df_dt - force_pred.T

    # ä½¿ç”¨ Huber Loss ä»£æ›¿ MSE
    # å½“è¯¯å·®å¤§æ—¶ï¼ˆé»‘å¤©é¹…ï¼‰ï¼Œåªäº§ç”Ÿçº¿æ€§çš„æ¢¯åº¦ï¼Œè€Œä¸æ˜¯å¹³æ–¹æ¢¯åº¦
    loss = torch.nn.HuberLoss(delta=delta)(residual, torch.zeros_like(residual))

    return loss
```

**å¯¹é½ Controllerï¼š**
- **Training:** Huber Loss å¿½ç•¥è®­ç»ƒæ•°æ®ä¸­çš„ Outliersã€‚
- **Trading:** Huber Potential å¿½ç•¥å®žæ—¶è¡Œæƒ…ä¸­çš„ Outliersã€‚
- **Result:** Training Objective å’Œ Trading Objective å®Œç¾Žç»Ÿä¸€ï¼ˆRobust Estimation & Controlï¼‰ã€‚

#### é€‰é¡¹3ï¼šå¯¹æ¯”å­¦ä¹ ï¼ˆæœ€çµæ´»ï¼‰

```python
# åŒä¸€ç°‡çš„è‚¡ç¥¨åº”è¯¥ç›¸ä¼¼ï¼Œä¸åŒç°‡çš„åº”è¯¥ä¸åŒ
def contrastive_loss(A_pred, f_series):
    # ç”¨A_predå®šä¹‰ç›¸ä¼¼åº¦
    # æœ€å¤§åŒ–åŒç°‡è‚¡ç¥¨çš„ç›¸ä¼¼åº¦
    # æœ€å°åŒ–ä¸åŒç°‡è‚¡ç¥¨çš„ç›¸ä¼¼åº¦
    pass
```

---

### 1.4 è¯„ä¼°æŒ‡æ ‡ï¼ˆæ›´æ–°ï¼‰

**æ ¸å¿ƒæŒ‡æ ‡ï¼šç°‡ä¸€è‡´æ€§**

```python
def evaluate_gnn_clustering(A_gnn, A_true, cluster_labels_true):
    """
    è¯„ä¼°GNNå­¦åˆ°çš„èšç±»è´¨é‡

    é‡ç‚¹ï¼š
    - NMI (Normalized Mutual Information)
    - ARI (Adjusted Rand Index)
    - ç°‡çº¯åº¦

    æ¬¡è¦ï¼š
    - è¾¹çš„F1ï¼ˆä¸æ˜¯æœ€é‡è¦çš„ï¼‰
    """
    # 1. ç”¨GNNå­¦åˆ°çš„å›¾è¿›è¡Œè°±èšç±»
    n_clusters = len(np.unique(cluster_labels_true))
    clustering = SpectralClustering(
        n_clusters=n_clusters,
        affinity='precomputed'
    )
    cluster_labels_gnn = clustering.fit_predict(A_gnn)

    # 2. è®¡ç®—ç°‡ä¸€è‡´æ€§
    nmi = normalized_mutual_info_score(cluster_labels_true, cluster_labels_gnn)
    ari = adjusted_rand_score(cluster_labels_true, cluster_labels_gnn)

    # 3. ç°‡çº¯åº¦
    purity = compute_purity(cluster_labels_true, cluster_labels_gnn)

    return {
        'nmi': nmi,  # æœ€é‡è¦
        'ari': ari,  # æœ€é‡è¦
        'purity': purity,
        # è¾¹çš„F1ä½œä¸ºå‚è€ƒ
        'edge_f1': compute_edge_f1(A_gnn, A_true)
    }
```

**é€šè¿‡æ ‡å‡†ï¼ˆæ›´æ–°ï¼‰ï¼š**
- âœ… **NMI > 0.7**ï¼ˆç°‡è¯†åˆ«å‡†ç¡®ï¼‰
- âœ… **ARI > 0.6**ï¼ˆç°‡åˆ†é…ä¸€è‡´ï¼‰
- âœ… **Purity > 0.8**ï¼ˆç°‡çº¯åº¦é«˜ï¼‰

è¾¹çš„F1ä¸æ˜¯ä¸»è¦æŒ‡æ ‡ï¼Œå› ä¸ºï¼š
- ç°‡å†…çš„å…·ä½“è¿žæŽ¥æ–¹å¼ä¸é‡è¦
- é‡è¦çš„æ˜¯èƒ½å¦è¯†åˆ«å‡ºå“ªäº›è‚¡ç¥¨å±žäºŽåŒä¸€ç°‡

---

### 1.5 å¯¹æ¯”æ–¹æ³•ï¼ˆæ›´æ–°ï¼‰

```python
methods = {
    'GNN': SimpleGNN(...),  # æˆ‘ä»¬çš„æ–¹æ³•
    'Han2021': han_et_al_clustering,  # Priority Baseline (Paper Replication)
    'Correlation': correlation_graph,  # Baseline 1
    'Dynamics': dynamics_based_graph,  # Baseline 2ï¼ˆä»…ç”¨äºŽéªŒè¯æ¦‚å¿µï¼‰
    'Spectral': spectral_clustering_on_correlation,  # Baseline 3
}
```

---

## Part 3: Baselineå¤çŽ° - Han et al. (2021)

**ç›®æ ‡ï¼š** å¤çŽ° Reference ä¸­çš„è®ºæ–‡æ–¹æ³•ï¼Œä½œä¸ºæ ¸å¿ƒBaseline
**æ–¹æ³•ç‰¹ç‚¹ï¼š** ç»“åˆåŸºæœ¬é¢æ•°æ®å’Œé‡ä»·æ•°æ®è¿›è¡Œèšç±»

### 3.1 ç‰¹å¾å·¥ç¨‹
éœ€è¦æž„å»ºä¸¤ç±»ç‰¹å¾ï¼š

1.  **é‡ä»·ç‰¹å¾ (Price-Volume)**
    - æ³¢åŠ¨çŽ‡ (Volatility)
    - æµåŠ¨æ€§ (Liquidity/Turnover)
    - åŠ¨é‡ (Momentum)
    - è´å¡” (Beta)

2.  **åŸºæœ¬é¢ç‰¹å¾ (Fundamental)**
    - ä¼°å€¼: P/E, P/B
    - è§„æ¨¡: Market Cap
    - ç›ˆåˆ©èƒ½åŠ›: ROE
    - æˆé•¿æ€§: Revenue Growth

**Goal:** å¤çŽ° "Fundamental Decomposition of Stock Returns" è®ºæ–‡æ–¹æ³•ï¼Œæž„å»ºåŸºäºŽåŸºæœ¬é¢çš„èšç±»åŸºçº¿ã€‚å³ä½¿æ˜¯ç®€å•çš„ K-Meansï¼Œæœ‰äº†æ­£ç¡®çš„åŸºæœ¬é¢ç‰¹å¾ï¼Œä¹Ÿèƒ½äº§ç”Ÿæžä½³çš„æ¿å—ç»“æž„ã€‚

#### 3.1 Data Pipeline & Utils
**Corrected Data Format (All files: Row=Time, Col=Tickers):**
*   **Daily Data:** `path/to/daily/{variable}/{year_month}.parquet` (e.g., `close`, `market_cap`).
*   **1-Min Data:** `path/to/1min/{variable}/{date}.parquet`.

**Required Raw Variables (for Han et al. Features):**
1.  **Price/Volume:** `close`, `open`, `high`, `low`, `volume`, `amount`.
    *   For: Volatility, Momentum (RSI), Turnover, Liquidity.
2.  **Fundamental:**
    *   `market_cap` (Size factor)
    *   `pe_ttm` (Valuation)
    *   `pb_lf` (Valuation)
    *   `roe_ttm` (Profitability)
    *   `operating_revenue_growth` (Growth)
    *   `industry_citic` (Ground Truth for validation)
    *   `co_filedate` (Precise Annual Availability)

### 3.1.2 Data Processing Improvements (User Requested)
1.  **Lazy Loading**: Load only relevant parquet files based on `start_date` and `end_date` (with buffer).
2.  **Precise Availability (`valid_from`)**:
    *   **Annual**: Use `comp.co_filedate`. Fallback: `datadate + 3 months`.
    *   **Quarterly**: Use `rdq`. Fallback: `datadate + 45 days`.
    *   **Monthly**: `date + 1 day`.
3.  **Data Cleaning**:
    *   Annual: Drop rows with `sales` is NaN.
    *   Quarterly: Drop rows with `salesq` is NaN.
    *   Monthly: Drop rows with `prc` is NaN.

### 3.1.2 Manual CUSIP Linking (Fallback Strategy)
Since `ccmxpf_linktable` is unavailable, we implement manual linking:
1.  **Compustat Data**: Ensure `comp_funda` includes `cusip` (from `comp.company` header or `funda`).
    *   *Note*: Header CUSIP (`comp.company`) is static (current), introducing some survivorship/look-ahead bias if ticker changed, but acceptable as fallback.
2.  **CRSP Data**: Use `crsp_stocknames` (downloaded in Step 5 of pull script).
    *   Columns: `permno`, `ncusip` (Historical CUSIP), `namedt`, `nameenddt`.
3.  **Merging Logic**:
    *   Match `funda.cusip` (first 6 digits) == `stocknames.ncusip` (first 6 digits).
    *   Filter: `funda.datadate` within `[stocknames.namedt, stocknames.nameenddt]`.
    *   Assign `permno` to Compustat rows.

#### 3.2 Methodology: Clustering Algorithms (The Baselines)
**Goal:** Evaluate standard unsupervised learning methods.
1.  **K-Means:** The standard. Rigid, spherical clusters.
2.  **DBSCAN:** Density-based. Can handle noise and non-spherical shapes. Important for filtering "outlier" stocks.
3.  **Agglomerative:** Hierarchical. useful for nested sector structures.

#### [Baseline] Han et al. (2021) Methodology Refined

> [!IMPORANT]
> **Revised Objective**: The goal is NOT to reproduce Industry Classification, but to find latent structural similarities that traditional sectors miss. We evaluate cluster validity using statistical metrics (Silhouette, Stability) and Trading Performance (Sharpe), rather than NMI against Industry.

#### 1. Experimental Modes
We will test three clustering configurations:
1.  **Fundamental Only**: Low-frequency (Quarterly/Monthly) metrics. Captures "Value/Growth" structure.
2.  **Price/Volume Only**: High-frequency aggregated metrics. Captures "Behavioral/Microstructure" structure.
3.  **Combined**: The fusion of both.

#### 2. Advanced Feature Engineering (Daily Microstructure)
We implement **16+ Distinct Features** in `src/data/daily_factors.py` using daily OHLCV and Trade Count data:

**A. Volatility & Distribution**
1.  `parkinson_vol`: High-Low range-based volatility estimator.
2.  `downside_vol`: Std of negative daily returns.
3.  `upside_vol`: Std of positive daily returns.
4.  `max_ret`: Maximum daily return in month (Lottery).
5.  `skew`: Skewness of daily returns.

**B. Liquidity & Volume**
6.  `amihud_illiquidity`: Mean(|Ret| / (Price * Volume)).
7.  `turnover_var`: Std of daily turnover.
8.  `dvol_cv`: Coeff of Variation of Dollar Volume (Liquidity Stability).
9.  `avg_trade_size`: Vol / NumTrades (Institutional Presence).
10. `illiq_numtrd`: Trade-based Illiquidity (Kyle's Lambda proxy).

**C. Microstructure & Efficiency**
11. `ret_gap_vol`: Volatility of overnight gaps (Split-adjusted).
12. `clv_mean`: Close Location Value (Accumulation/Distribution proxy).
13. `zero_ret_pct`: Percentage of days with zero return.
14. `payout_yield`: Sum(Return - Retx) (Realized Dividend Yield).
15. `intraday_ret`: Mean((Close/Open) - 1).
16. `intraday_vol`: Std((Close/Open) - 1).

#### 3.3 Comparison Metrics
*   **Cluster Quality:** NMI against true Sector Labels.
*   **Trading Performance:** Use this $A$ in our *Deep Potential Controller*.
    *   Does Fundamental $A$ generate higher IC than Price Correlation $A$?
    *   Hypothesis: Yes, especially in "Diversified" regimes.
### 3.2 èšç±»ç®—æ³•
é€šå¸¸æµç¨‹ï¼š
1.  ç‰¹å¾æ ‡å‡†åŒ– (Z-score)
2.  é™ç»´ (PCA)
3.  èšç±» (DBSCAN/OPTICS/K-Means)

### 3.3 å®žæ–½è®¡åˆ’
- [ ] **Data Loader**: æ‰©å……æ•°æ®åŠ è½½å™¨ä»¥æ”¯æŒåŸºæœ¬é¢æ•°æ®
- [ ] **Feature Extractor**: å®žçŽ°å¸¸ç”¨çš„å› å­è®¡ç®—
- [ ] **Clustering Model**: å®žçŽ°è®ºæ–‡ä¸­çš„èšç±»æµç¨‹
- [ ] **Comparison**: åœ¨Part 1å’ŒPart 2ä¸­åŠ å…¥æ­¤æ–¹æ³•çš„å¯¹æ¯”

---

## Part 4: å‡è®¾2éªŒè¯ - è‚¡ç¥¨å¸‚åœºå­˜åœ¨æ‹‰æ™®æ‹‰æ–¯åŠ¨åŠ›å­¦

### 2.1 æ ¸å¿ƒæµ‹è¯•ï¼ˆä¿æŒä¸å˜ï¼‰

**æµ‹è¯•ï¼š** æ¢å¤åŠ› $F = -2Lf$ æ˜¯å¦é¢„æµ‹æœªæ¥æ”¶ç›Š

```python
def test_laplacian_dynamics_in_market(f_series, L):
    """
    æµ‹è¯•å¸‚åœºæ˜¯å¦éµå¾ªæ‹‰æ™®æ‹‰æ–¯åŠ¨åŠ›å­¦

    æ ¸å¿ƒï¼šF_t ä¸Ž Î”f_{t+1} çš„ç›¸å…³æ€§
    """
    ic_results = test_force_return_correlation(f_series, L, lag=1)

    # é€šè¿‡æ ‡å‡†
    return {
        'ic_mean': ic_results['ic_mean'],  # > 0.05
        'p_value': ic_results['p_value'],  # < 0.05
        'passed': (ic_results['ic_mean'] > 0.05 and
                  ic_results['p_value'] < 0.05)
    }
```

### 2.2 ä¸åŒæ—¶é—´å°ºåº¦æµ‹è¯•

```python
# æµ‹è¯•ä¸åŒé¢‘çŽ‡çš„æ•°æ®
time_scales = {
    '1min': load_1min_data(),
    '5min': load_5min_data(),
    '1hour': load_1hour_data(),
    '1day': load_1day_data(),
}

for scale, data in time_scales.items():
    result = test_laplacian_dynamics_in_market(data, L)
    print(f"{scale}: IC = {result['ic_mean']:.4f}")
```

**ç›®æ ‡ï¼š** æ‰¾åˆ°æ‹‰æ™®æ‹‰æ–¯åŠ¨åŠ›å­¦æœ€æ˜¾è‘—çš„æ—¶é—´å°ºåº¦

---

## å®žæ–½æ–‡ä»¶ç»“æž„ï¼ˆæ›´æ–°ï¼‰

```
src/validation/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ synthetic_data.py          # âœ… å·²å®žçŽ°
â”œâ”€â”€ gnn_models.py              # ðŸ†• GNNæ¨¡åž‹å®šä¹‰
â”‚   â”œâ”€â”€ SimpleGNN
â”‚   â””â”€â”€ AdvancedGNN (å¯é€‰)
â”‚
â”œâ”€â”€ gnn_training.py            # ðŸ†• GNNè®­ç»ƒé€»è¾‘
â”‚   â”œâ”€â”€ train_gnn_supervised()
â”‚   â”œâ”€â”€ train_gnn_dynamics()
â”‚   â””â”€â”€ train_gnn_contrastive()
â”‚
â”œâ”€â”€ graph_learning.py          # âœ… å·²å®žçŽ°ï¼ˆä½œä¸ºbaselineï¼‰
â”‚   â”œâ”€â”€ correlation_graph()
â”‚   â”œâ”€â”€ dynamics_based_graph()  # ä»…ç”¨äºŽæ¦‚å¿µéªŒè¯
â”‚   â””â”€â”€ spectral_clustering()
â”‚
â”œâ”€â”€ dynamics_test.py           # âœ… å·²å®žçŽ°
â”œâ”€â”€ metrics.py                 # ðŸ”„ éœ€æ›´æ–°ï¼ˆå¼ºè°ƒç°‡æŒ‡æ ‡ï¼‰
â””â”€â”€ run_validation.py          # ðŸ”„ éœ€æ›´æ–°ï¼ˆGNNä¸ºä¸»ï¼‰

### Design Decisions: Baseline Replication (Updated)

### Addressing Data Frequency Imbalance (Monthly vs Daily)
**User Question**: "How to mix frequencies without one dominating?"
**Solution**: **PCA (Principal Component Analysis)**.
*   **Mechanism**: We do NOT cluster on the 116 raw features (100 Monthly + 16 Daily). Instead, we cluster on the Top $K$ Principal Components.
*   **Why it works**:
    *   The 100+ Monthly Fundamental factors are highly collinear (Value, Growth, Profitability groups). PCA compresses this redundancy into a few "Fundamental Components".
    *   The 16 Daily Microstructure factors are distinct. They form "Microstructure Components".
    *   **Result**: The clustering inputs are orthogonal Risk Factors, balanced by *Information Content (Variance)*, not by the raw count of columns.

### Clustering Models (To Be Implemented)
1.  **K-Means**: Standard baseline. Simple, rigid clusters.
2.  **DBSCAN**: Density-based. Crucial for filtering noise stocks that don't follow any group.
3.  **OPTICS**: Advanced density clustering, better for varying density.
4.  **Agglomerative**: Hierarchical clustering.

### Baseline Pipeline Workflow (Rolling Walk-Forward)
Strictly implementing Han et al.'s rolling methodology:
1.  **Iterative Loop**: For each Month $M$ in Test Range:
    *   **Train Data**: Rolling window $[M - \text{Lookback}, M - 1]$.
    *   **Preprocessing**:
        *   Clean NaNs (Drop sparse, Impute 0).
        *   Remove Constant Features.
        *   **StandardScaler** (Fit on Train).
        *   **PCA** (Fit on Train) -> Extract Risk Factors.
    *   **Clustering**: Fit Model (K-Means/DBSCAN/etc) on PCA Components.
    *   **Prediction**: Apply fitted Scaler/PCA/Clusterer to Month $M$ data.
    *   **Trading**: Generate signals based on Mean Reversion ($R_i - R_{cluster}$).
    *   **Store**: Save Month $M$ PnL and Signals.
2.  **Evaluation**: Concatenate all monthly results and compute global metrics.
```

---

## éªŒè¯è®¡åˆ’ï¼ˆæ›´æ–°ï¼‰

### Week 1: GNNå®žçŽ°å’Œåˆæˆæ•°æ®æµ‹è¯•

**Day 1-2: å®žçŽ°GNNæ¨¡åž‹**
- [ ] `gnn_models.py` - SimpleGNN
- [ ] `gnn_training.py` - åŠ¨åŠ›å­¦ä¸€è‡´æ€§loss
- [ ] æµ‹è¯•ï¼šèƒ½å¦åœ¨åˆæˆæ•°æ®ä¸Šæ”¶æ•›

**Day 3-4: è®­ç»ƒå’Œè¯„ä¼°**
- [ ] è®­ç»ƒGNN onåˆæˆæ•°æ®
- [ ] è¯„ä¼°ç°‡è¯†åˆ«è´¨é‡ï¼ˆNMI, ARIï¼‰
- [ ] å¯¹æ¯”baselineæ–¹æ³•

**Day 5-7: è°ƒä¼˜å’ŒéªŒè¯**
- [ ] è°ƒæ•´è¶…å‚æ•°
- [ ] æµ‹è¯•ä¸åŒå™ªå£°æ°´å¹³
- [ ] ç”ŸæˆæŠ¥å‘Šï¼šNMI > 0.7?

### Week 2: çœŸå®žæ•°æ®éªŒè¯

**Day 1-2: æ•°æ®å‡†å¤‡**
- [ ] ä¸‹è½½è‚¡ç¥¨æ•°æ®ï¼ˆXLKç­‰ï¼‰
- [ ] é¢„å¤„ç†ï¼šåŽ»è¶‹åŠ¿ã€æ ‡å‡†åŒ–

**Day 3-5: åŠ¨åŠ›å­¦æµ‹è¯•**
- [ ] æµ‹è¯•ä¸åŒæ—¶é—´å°ºåº¦
- [ ] ICæµ‹è¯•ï¼šæ˜¯å¦å­˜åœ¨æ‹‰æ™®æ‹‰æ–¯åŠ¨åŠ›å­¦
- [ ] æ‰¾åˆ°æœ€ä¼˜æ—¶é—´å°ºåº¦

**Day 6-7: GNNåº”ç”¨**
- [ ] ç”¨GNNå­¦ä¹ çœŸå®žæ•°æ®çš„å›¾
- [ ] éªŒè¯å­¦åˆ°çš„ç°‡æ˜¯å¦ç¨³å®š
- [ ] æœ€ç»ˆå†³ç­–ï¼šGo or No-Go

---

## æˆåŠŸæ ‡å‡†ï¼ˆæ›´æ–°ï¼‰

### å‡è®¾1ï¼ˆGNNèšç±»ï¼‰é€šè¿‡æ ‡å‡†ï¼š
- âœ… åˆæˆæ•°æ®ï¼š**NMI > 0.7, ARI > 0.6**
- âœ… GNNä¼˜äºŽcorrelation baselineï¼ˆNMIæå‡ > 0.1ï¼‰
- âœ… ç°‡ç¨³å®šæ€§ï¼šæ»šåŠ¨çª—å£ç›¸ä¼¼åº¦ > 0.6

### å‡è®¾2ï¼ˆåŠ¨åŠ›å­¦ï¼‰é€šè¿‡æ ‡å‡†ï¼š
- âœ… åˆæˆæ•°æ®ï¼šIC > 0.3 (p < 0.01)
- âœ… çœŸå®žæ•°æ®ï¼š**IC > 0.05 (p < 0.05)**
- âœ… è‡³å°‘ä¸€ä¸ªæ—¶é—´å°ºåº¦æ˜¾è‘—

### ç»¼åˆå†³ç­–ï¼š
- **ç»¿ç¯ï¼ˆç»§ç»­ï¼‰ï¼š** ä¸¤ä¸ªå‡è®¾éƒ½é€šè¿‡
- **é»„ç¯ï¼ˆä¿®æ­£ï¼‰ï¼š** ä¸€ä¸ªå‡è®¾é€šè¿‡
- **çº¢ç¯ï¼ˆåœæ­¢ï¼‰ï¼š** ä¸¤ä¸ªå‡è®¾éƒ½ä¸é€šè¿‡

---

## å…³é”®åŒºåˆ«æ€»ç»“

| æ–¹é¢ | æ—§ç†è§£ | æ–°ç†è§£ï¼ˆæ­£ç¡®ï¼‰ |
|-----|--------|--------------|
| ä¸»è¦æ–¹æ³• | dynamics_based_graph | **GNNå­¦ä¹ å›¾** |
| éªŒè¯é‡ç‚¹ | è¾¹çš„F1 | **ç°‡çš„NMI/ARI** |
| dynamics_based_graphçš„ä½œç”¨ | ä¸»è¦æ–¹æ³• | Baseline/æ¦‚å¿µéªŒè¯ |
| æ ¸å¿ƒå‡è®¾ | èƒ½ä»ŽåŠ¨åŠ›å­¦åæŽ¨å›¾ | **GNNèƒ½å­¦åˆ°åŠ¨åŠ›å­¦ç°‡** |

---

## ä¸‹ä¸€æ­¥

1. **å®žçŽ°GNNæ¨¡åž‹**ï¼ˆ`gnn_models.py`ï¼‰
2. **å®žçŽ°è®­ç»ƒé€»è¾‘**ï¼ˆ`gnn_training.py`ï¼‰
3. **æ›´æ–°è¯„ä¼°æŒ‡æ ‡**ï¼ˆå¼ºè°ƒNMI/ARIï¼‰
4. **æ›´æ–°éªŒè¯è„šæœ¬**ï¼ˆGNNä¸ºä¸»ï¼Œdynamics_based_graphä¸ºbaselineï¼‰

è¿™ä¸ªé€»è¾‘é“¾æ¡æ›´æ¸…æ™°ï¼š
```
æ‹‰æ™®æ‹‰æ–¯åŠ¨åŠ›å­¦ â†’ å½¢æˆç°‡ â†’ GNNå­¦ä¹ ç°‡ â†’ Controlleråˆ©ç”¨åŠ¿èƒ½äº¤æ˜“
```
